<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TolkienGPT Architecture</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1e3c72, #2a5298);
            color: white;
            margin: 0;
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 30px;
            backdrop-filter: blur(10px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }
        
        h1 {
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 30px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
        }
        
        .architecture {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
            margin: 40px 0;
        }
        
        .layer {
            background: rgba(255, 255, 255, 0.15);
            border: 2px solid rgba(255, 255, 255, 0.3);
            border-radius: 12px;
            padding: 15px 25px;
            min-width: 300px;
            text-align: center;
            position: relative;
            backdrop-filter: blur(5px);
            transition: all 0.3s ease;
        }
        
        .layer:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(255, 255, 255, 0.2);
        }
        
        .input-layer { background: linear-gradient(45deg, #ff6b6b, #ee5a24); }
        .embedding-layer { background: linear-gradient(45deg, #4834d4, #686de0); }
        .transformer-layer { background: linear-gradient(45deg, #00d2d3, #01a3a4); }
        .attention-layer { background: linear-gradient(45deg, #feca57, #ff9ff3); }
        .mlp-layer { background: linear-gradient(45deg, #48dbfb, #0abde3); }
        .norm-layer { background: linear-gradient(45deg, #1dd1a1, #10ac84); }
        .output-layer { background: linear-gradient(45deg, #ff9ff3, #feca57); }
        
        .layer-title {
            font-weight: bold;
            font-size: 1.3em;
            margin-bottom: 8px;
        }
        
        .layer-desc {
            font-size: 0.9em;
            opacity: 0.9;
            line-height: 1.4;
        }
        
        .arrow {
            font-size: 2em;
            color: #feca57;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
        }
        
        .specs {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 40px;
        }
        
        .spec-card {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
            padding: 20px;
            text-align: center;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        
        .spec-title {
            font-weight: bold;
            font-size: 1.1em;
            margin-bottom: 10px;
            color: #feca57;
        }
        
        .tokenization {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .token-example {
            display: inline-block;
            background: #4834d4;
            padding: 5px 10px;
            margin: 3px;
            border-radius: 5px;
            font-family: monospace;
            font-size: 0.9em;
        }
        
        .block-detail {
            background: rgba(0, 0, 0, 0.2);
            border-radius: 8px;
            padding: 15px;
            margin-top: 15px;
            border-left: 4px solid #feca57;
        }
        
        .multi-head {
            display: flex;
            justify-content: space-around;
            margin-top: 10px;
        }
        
        .head {
            background: rgba(255, 255, 255, 0.2);
            border-radius: 5px;
            padding: 8px;
            font-size: 0.8em;
            min-width: 40px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üßô‚Äç‚ôÇÔ∏è TolkienGPT Architecture</h1>
        
        <div class="tokenization">
            <h3>üî§ 2-Character Tokenization Strategy</h3>
            <p><strong>Input Text:</strong> "In a hole in the ground there lived a hobbit"</p>
            <p><strong>Tokens:</strong> 
                <span class="token-example">In</span>
                <span class="token-example"> a</span>
                <span class="token-example"> h</span>
                <span class="token-example">ol</span>
                <span class="token-example">e </span>
                <span class="token-example">in</span>
                <span class="token-example"> t</span>
                <span class="token-example">he</span>
                <span class="token-example"> g</span>
                <span class="token-example">ro</span>
                <span class="token-example">un</span>
                <span class="token-example">d </span>
                <span class="token-example">th</span>
                <span class="token-example">er</span>
                <span class="token-example">e </span>
                <span class="token-example">li</span>
                <span class="token-example">ve</span>
                <span class="token-example">d </span>
                <span class="token-example">a </span>
                <span class="token-example">ho</span>
                <span class="token-example">bb</span>
                <span class="token-example">it</span>
            </p>
        </div>
        
        <div class="architecture">
            <!-- Input -->
            <div class="layer input-layer">
                <div class="layer-title">üìù Input Layer</div>
                <div class="layer-desc">Token IDs: [1247, 891, 1055, ...]<br>Shape: (batch_size, sequence_length)</div>
            </div>
            
            <div class="arrow">‚Üì</div>
            
            <!-- Token + Position Embeddings -->
            <div class="layer embedding-layer">
                <div class="layer-title">üéØ Embedding Layer</div>
                <div class="layer-desc">
                    <strong>Token Embeddings:</strong> vocab_size ‚Üí n_embd (2019 ‚Üí 96)<br>
                    <strong>Position Embeddings:</strong> block_size ‚Üí n_embd (512 ‚Üí 96)<br>
                    <strong>Combined:</strong> tok_emb + pos_emb + dropout
                </div>
            </div>
            
            <div class="arrow">‚Üì</div>
            
            <!-- Transformer Blocks (6x) -->
            <div class="layer transformer-layer">
                <div class="layer-title">üîÑ Transformer Blocks (6 Layers)</div>
                <div class="layer-desc">Each block contains:</div>
                
                <div class="block-detail">
                    <!-- Layer Norm 1 -->
                    <div class="layer norm-layer" style="margin-bottom: 10px;">
                        <div class="layer-title">üîß Layer Norm 1</div>
                        <div class="layer-desc">Pre-normalization before attention</div>
                    </div>
                    
                    <!-- Multi-Head Attention -->
                    <div class="layer attention-layer" style="margin-bottom: 10px;">
                        <div class="layer-title">üéØ Multi-Head Causal Self-Attention</div>
                        <div class="layer-desc">6 attention heads, head_dim = 16<br>Query, Key, Value projections with causal masking</div>
                        <div class="multi-head">
                            <div class="head">Head 1</div>
                            <div class="head">Head 2</div>
                            <div class="head">Head 3</div>
                            <div class="head">Head 4</div>
                            <div class="head">Head 5</div>
                            <div class="head">Head 6</div>
                        </div>
                    </div>
                    
                    <!-- Residual Connection 1 -->
                    <div style="text-align: center; margin: 10px 0; color: #feca57;">
                        <strong>+ Residual Connection</strong>
                    </div>
                    
                    <!-- Layer Norm 2 -->
                    <div class="layer norm-layer" style="margin-bottom: 10px;">
                        <div class="layer-title">üîß Layer Norm 2</div>
                        <div class="layer-desc">Pre-normalization before MLP</div>
                    </div>
                    
                    <!-- MLP -->
                    <div class="layer mlp-layer" style="margin-bottom: 10px;">
                        <div class="layer-title">üß† Multi-Layer Perceptron (MLP)</div>
                        <div class="layer-desc">
                            Linear(96 ‚Üí 384) ‚Üí GELU ‚Üí Linear(384 ‚Üí 96)<br>
                            4x expansion factor with dropout
                        </div>
                    </div>
                    
                    <!-- Residual Connection 2 -->
                    <div style="text-align: center; margin: 10px 0; color: #feca57;">
                        <strong>+ Residual Connection</strong>
                    </div>
                </div>
            </div>
            
            <div class="arrow">‚Üì</div>
            
            <!-- Final Layer Norm -->
            <div class="layer norm-layer">
                <div class="layer-title">üîß Final Layer Normalization</div>
                <div class="layer-desc">Final normalization before output projection</div>
            </div>
            
            <div class="arrow">‚Üì</div>
            
            <!-- Output Head -->
            <div class="layer output-layer">
                <div class="layer-title">üì§ Language Model Head</div>
                <div class="layer-desc">
                    Linear projection: n_embd ‚Üí vocab_size (96 ‚Üí 2019)<br>
                    <strong>Weight Tying:</strong> Shared with input embeddings<br>
                    Softmax ‚Üí Next token probabilities
                </div>
            </div>
            
            <div class="arrow">‚Üì</div>
            
            <!-- Generation -->
            <div class="layer input-layer">
                <div class="layer-title">üé≠ Text Generation</div>
                <div class="layer-desc">
                    Nucleus sampling (top-p=0.9) + Temperature scaling<br>
                    Autoregressive generation with context window sliding
                </div>
            </div>
        </div>
        
        <!-- Model Specifications -->
        <div class="specs">
            <div class="spec-card">
                <div class="spec-title">üìä Model Size</div>
                <div>~1.2M Parameters</div>
                <div>6 Transformer Layers</div>
                <div>6 Attention Heads</div>
            </div>
            
            <div class="spec-card">
                <div class="spec-title">üéØ Dimensions</div>
                <div>Embedding: 96</div>
                <div>Context: 512 tokens</div>
                <div>Head dim: 16</div>
                <div>MLP hidden: 384</div>
            </div>
            
            <div class="spec-card">
                <div class="spec-title">üìö Vocabulary</div>
                <div>2,019 2-char tokens</div>
                <div>Handles archaic spellings</div>
                <div>Preserves punctuation</div>
                <div>UNK token for rare pairs</div>
            </div>
            
            <div class="spec-card">
                <div class="spec-title">‚öôÔ∏è Training</div>
                <div>AdamW optimizer</div>
                <div>Cosine LR scheduling</div>
                <div>Gradient clipping</div>
                <div>Dropout: 0.1</div>
            </div>
        </div>
        
        <div style="text-align: center; margin-top: 40px; font-size: 0.9em; opacity: 0.8;">
            <p>üßô‚Äç‚ôÇÔ∏è Architecture optimized for Tolkien's distinctive linguistic patterns</p>
            <p>Character-level tokenization captures archaic forms, invented words, and unique punctuation</p>
        </div>
    </div>
</body>
</html>